{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/skmultiflow_dev/lib/python3.6/site-packages/scikit_multiflow-0.5.dev0-py3.6-macosx-10.9-x86_64.egg/skmultiflow/trees/hoeffding_tree.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from skmultiflow.trees.hoeffding_adaptive_tree import HoeffdingAdaptiveTreeClassifier\n",
    "from skmultiflow.data.file_stream import FileStream\n",
    "from skmultiflow.evaluation.evaluate_prequential import EvaluatePrequential\n",
    "import numpy as np\n",
    "import os\n",
    "from skmultiflow.trees.hoeffding_tree import HoeffdingTreeClassifier\n",
    "print (inspect.getfile(HoeffdingTreeClassifier))  \n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_feature_names=[     'amount_usd',      \n",
    "                            'client_age',\n",
    "                           #'client_gender__',\n",
    "                          'client_gender__F',\n",
    "                          'client_gender__M',\n",
    "                            'debit_type__TC',\n",
    "                            'debit_type__TD',\n",
    "                          #'agency_region__0',\n",
    "                          'agency_region__1',\n",
    "                          'agency_region__2',\n",
    "                          'agency_region__3',\n",
    "                          'agency_region__4',\n",
    "                          'agency_region__5',\n",
    "                          'agency_region__6',\n",
    "                          'agency_region__7',\n",
    "                          'agency_region__8',\n",
    "                          'agency_region__9',\n",
    "                         'agency_region__10',\n",
    "                         'agency_region__11',\n",
    "                         'agency_region__12',\n",
    "                  #'merchant_departement__99',\n",
    "            'merchant_departement__AMAZONAS',\n",
    "              'merchant_departement__ANCASH',\n",
    "            'merchant_departement__APURIMAC',\n",
    "            'merchant_departement__AREQUIPA',\n",
    "            'merchant_departement__AYACUCHO',\n",
    "           'merchant_departement__CAJAMARCA',\n",
    "              'merchant_departement__CALLAO',\n",
    "               'merchant_departement__CUSCO',\n",
    "        'merchant_departement__HUANCAVELICA',\n",
    "             'merchant_departement__HU√ÅNUCO',\n",
    "                 'merchant_departement__ICA',\n",
    "               'merchant_departement__JUNIN',\n",
    "         'merchant_departement__LA LIBERTAD',\n",
    "          'merchant_departement__LAMBAYEQUE',\n",
    "                'merchant_departement__LIMA',\n",
    "              'merchant_departement__LORETO',\n",
    "       'merchant_departement__MADRE DE DIOS',\n",
    "            'merchant_departement__MOQUEGUA',\n",
    "               'merchant_departement__PASCO',\n",
    "               'merchant_departement__PIURA',\n",
    "                'merchant_departement__PUNO',\n",
    "          'merchant_departement__SAN MARTIN',\n",
    "               'merchant_departement__TACNA',\n",
    "              'merchant_departement__TUMBES',\n",
    "             'merchant_departement__UCAYALI',\n",
    "                              'social_class']\n",
    "\n",
    "#df = pd.read_csv('../data_output/export_dataframe_0v4.csv',header=None,names=dataset_feature_names)\n",
    "#df = pd.read_csv('../data_output/balanced.csv',header=None,names=dataset_feature_names)\n",
    "#df.sum()/1000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate adnostic uniform sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_adnostic_sample():\n",
    "    \n",
    "    dict_aux = {}\n",
    "    mu = 0.0 \n",
    "    sigma = 1.0\n",
    "    aux = random.uniform(mu, sigma)\n",
    "    dict_aux['amount_usd']= aux*-1 if (aux<0) else aux\n",
    "    aux = random.uniform(mu, sigma)\n",
    "    dict_aux['client_age']= aux*-1 if (aux<0) else aux\n",
    "    \n",
    "    aux = random.uniform(mu, sigma)\n",
    "    dict_aux['client_gender']=1 if (int(round(aux,0)) == 1 ) else 0\n",
    "    \n",
    "    aux = random.uniform(mu, sigma)\n",
    "    dict_aux['debit_type']=1 if (int(round(aux,0)) == 1 ) else 0\n",
    "        \n",
    "    aux = random.uniform(mu, sigma)\n",
    "    dict_aux['agency_region']= int(round(aux * 12,0))\n",
    "    \n",
    "    aux = random.uniform(mu, sigma)        \n",
    "    dict_aux['merchant_departement']= int(round(aux * 25,0))\n",
    "        \n",
    "    return dict_aux\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_sample_from_original(pX,py,oracle,minority_label,original=False):\n",
    "    size = len(py)\n",
    "    #size = 2000\n",
    "    print(\"generate_gaussian_original minority nb: \",size )    \n",
    "    i = 0\n",
    "    #mylist = []\n",
    "    X_aux = []\n",
    "    \n",
    "    while (i<size):\n",
    "        \n",
    "        row = pX[i].copy()\n",
    "        #print (type(row))\n",
    "        i+=1\n",
    "        if (not original):\n",
    "            #print (row,i)\n",
    "            if (np.random.randint(2) == 1):\n",
    "                mu = 0.006903\n",
    "                sigma = 0.017028\n",
    "                aux = random.uniform(mu, sigma)\n",
    "                row[0] += aux\n",
    "            elif (np.random.randint(2) == 1):\n",
    "                mu = 0.006903\n",
    "                sigma = 0.017028\n",
    "                aux = random.uniform(mu, sigma)\n",
    "                row[0] += aux\n",
    "                mu = 0.343192\n",
    "                sigma = 0.144008\n",
    "                aux = random.uniform( sigma,mu)\n",
    "                row[1] += aux\n",
    "            else:\n",
    "                mu = 0.343192\n",
    "                sigma = 0.144008\n",
    "                #aux = random.gauss(mu, sigma)\n",
    "                aux = random.uniform( sigma,mu)\n",
    "                row[1] += aux\n",
    "                \n",
    "        \n",
    "        y_iter = oracle.predict( np.asarray( [row] ) )\n",
    "\n",
    "        if (y_iter[0]  == 1):       \n",
    "            #print (\"prediction: \",y_iter,i)\n",
    "            row = np.append(row,[1],axis=0)\n",
    "            X_aux.append (row)\n",
    "    #y_res = oracle.predict( np.asarray(X_aux) )\n",
    "    #print (\"oracle.predict(X) : \", y_res)\n",
    "    #print (np.unique(y_res,return_counts=True) )\n",
    "    return pd.DataFrame(X_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_sample_from_original(pX,py,oracle,minority_label,original=False):\n",
    "    xmin = np.min(pX,axis=0)\n",
    "    xman = np.max(pX,axis=0)\n",
    "    x0_min =  xmin[0]\n",
    "    x1_min =  xmin[1]\n",
    "    x0_max =  xman[0]\n",
    "    x1_max =  xman[1] \n",
    "    print (x0_min,x1_min,x0_max,x1_max)\n",
    "    size = len(py)\n",
    "    #size = 2000\n",
    "    print(\"generate_uniform_from_original minority nb: \",size )    \n",
    "    i = 0\n",
    "    #mylist = []\n",
    "    X_aux = []\n",
    "    \n",
    "    while (i<size):\n",
    "        \n",
    "        row = pX[i].copy()\n",
    "        #print (type(row))\n",
    "        i+=1\n",
    "        if (not original):\n",
    "            #print (row,i)\n",
    "            if (np.random.randint(2) == 1):\n",
    "                aux = random.uniform(x0_min, x0_max)\n",
    "                row[0] += aux\n",
    "            elif (np.random.randint(2) == 1):\n",
    "                aux = random.uniform(x0_min, x0_max)\n",
    "                row[0] += aux\n",
    "                aux = random.uniform( x1_min,x1_max)\n",
    "                row[1] += aux\n",
    "            else:\n",
    "                aux = random.uniform( x1_min,x1_max)\n",
    "                row[1] += aux\n",
    "                \n",
    "        \n",
    "        y_iter = oracle.predict( np.asarray( [row] ) )\n",
    "        \n",
    "        if (y_iter[0]  == 1):       \n",
    "            #print (\"prediction: \",y_iter,i)\n",
    "            X_aux.append (row)\n",
    "    #y_res = oracle.predict( np.asarray(X_aux) )\n",
    "    #print (\"oracle.predict(X) : \", y_res)\n",
    "    #print (np.unique(y_res,return_counts=True) )\n",
    "    return pd.DataFrame(X_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_syntethic_data (nb_samples,pX,py,oracle,minority_label,pmax_iterations=10):\n",
    "    \n",
    "    newdf = pd.DataFrame()\n",
    "    \n",
    "    #Minority class\n",
    "    print (\"generating....\")\n",
    "    sampled_df = generate_gaussian_sample_from_original(pX,py,oracle,pmax_iterations)\n",
    "    nb_sampled = sampled_df.shape[0]\n",
    "    print(\"nb_sampled: \",sampled_df.shape)\n",
    "    \n",
    "    ii = 0\n",
    "    print (\"while....\")\n",
    "    if (sampled_df.shape[0]<=(nb_samples/2)):\n",
    "        #print (sampled_df.shape[0],\" < \",(nb_samples/2))\n",
    "        #print (\"max_iterations: \",pmax_iterations)\n",
    "        while   (ii < pmax_iterations):    \n",
    "            #print ('i : ',ii)\n",
    "            sampled_df_aux = generate_gaussian_sample_from_original(pX,py,oracle,pmax_iterations)\n",
    "            sampled_df = sampled_df.append(sampled_df_aux, ignore_index = True) \n",
    "            #print (sampled_df_aux.shape)\n",
    "            #print (sampled_df.shape)\n",
    "            ii+=1\n",
    "        if (sampled_df.shape[0]<=(nb_samples/2)):\n",
    "            ii = 0\n",
    "            print (\"second sampling\")\n",
    "            while   (ii < pmax_iterations  ):\n",
    "                sampled_df_aux = generate_gaussian_sample_from_original(pX,py,oracle,pmax_iterations,original=True)\n",
    "                sampled_df = sampled_df.append(sampled_df_aux, ignore_index = False) \n",
    "                print (sampled_df.shape)\n",
    "                ii+=1\n",
    "                if (sampled_df.shape[0]>=(nb_samples/2)):\n",
    "                    print (\"Break : \", sampled_df.shape)\n",
    "                    break\n",
    "    if (sampled_df.shape[0] >= int(nb_samples/2) ):\n",
    "        newdf = sampled_df.iloc[0:int(nb_samples/2)]#, ignore_index = True)\n",
    "    else :\n",
    "        newdf = sampled_df\n",
    "    #newdf = sampled_df.iloc[0:int(nb_samples/2)]#, ignore_index = True)\n",
    "    #newdf.append(sampled_df, ignore_index = True)\n",
    "    \n",
    "    print (\"sampled_df shape\", sampled_df.shape)\n",
    "    print (\"sampled_df cols\", sampled_df.columns)\n",
    "    \n",
    "    #All classes\n",
    "    \n",
    "    i = 0\n",
    "    column_names = ['amount_usd',\n",
    "                    'client_age',\n",
    "                    'client_gender',     \n",
    "                    'debit_type',        \n",
    "                    'agency_region',     \n",
    "                    'merchant_departement']\n",
    "    df = pd.DataFrame(columns = column_names)\n",
    "    while (i < (nb_samples/2)):\n",
    "        row ={}\n",
    "        row = generate_uniform_adnostic_sample()\n",
    "        df = df.append(row  , ignore_index=True)\n",
    "        i+=1\n",
    "        \n",
    "    df_client_gender_henc = pd.get_dummies(df['client_gender'], prefix = 'client_gender_')\n",
    "    df_debit_type_henc = pd.get_dummies(df['debit_type'], prefix = 'debit_type_')\n",
    "    df_agency_region_henc = pd.get_dummies(df['agency_region'], prefix = 'agency_region_')\n",
    "    df_merchant_departement_henc = pd.get_dummies(df['merchant_departement'], prefix = 'merchant_departement_')\n",
    "        \n",
    "    del df['client_gender']\n",
    "    del df['debit_type']\n",
    "    del df['agency_region']\n",
    "    del df['merchant_departement']\n",
    "    \n",
    "    \n",
    "    newdf2 = pd.concat([df, \n",
    "                   df_client_gender_henc,\n",
    "                   df_debit_type_henc,\n",
    "                   df_agency_region_henc,\n",
    "                   df_merchant_departement_henc\n",
    "                  ], axis=1)\n",
    "    \n",
    "    print (\"newdf2 shape\", newdf2.shape)\n",
    "    print (\"newdf2 cols\", newdf2.columns)\n",
    "    \n",
    "    newdf2 = newdf2[newdf2['agency_region__12.0'] != 1]\n",
    "    newdf2 = newdf2[newdf2['merchant_departement__0.0'] != 1]\n",
    "        \n",
    "    del newdf2['agency_region__12.0']\n",
    "    del newdf2['merchant_departement__0.0']\n",
    "    aX=np.asarray(newdf2)\n",
    "    aY=oracle.predict(aX)\n",
    "    newdf2['social_class']=aY\n",
    "    newdf2.columns = list(range(0,44))\n",
    "    \n",
    "    newdf = newdf.append(newdf2)\n",
    "    print (\"newdf\", newdf.columns)\n",
    "    \n",
    "    newdf = newdf.sample(frac = 1)\n",
    "    \n",
    "    return newdf #sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_y_weight(y):\n",
    "    aux = np.unique(y,return_counts=True)\n",
    "    nb_classes = len(aux[0])\n",
    "    list_classes = aux[0]\n",
    "    count_classes = aux[1]\n",
    "    sum_classes =  float(np.sum(count_classes))\n",
    "    weight_classes = count_classes/sum_classes\n",
    "    y_weights = y.copy()\n",
    "    \n",
    "    i = 0\n",
    "    while i < nb_classes:\n",
    "        \n",
    "        class_i = list_classes[i]\n",
    "        y_weights = np.where(y_weights==float(class_i), weight_classes[i], y_weights) \n",
    "        i += 1\n",
    "    return y_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([776619, 134890]))\n",
      "HoeffdingTreeClassifier class constructor\n",
      "HoeffdingTreeClassifier class constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HoeffdingAdaptiveTreeClassifier(binary_split=True, bootstrap_sampling=True,\n",
       "                                grace_period=2000, leaf_prediction='nba',\n",
       "                                max_byte_size=33554432,\n",
       "                                memory_estimate_period=1000000, nb_threshold=0,\n",
       "                                no_preprune=False,\n",
       "                                nominal_attributes=[2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
       "                                                    11, 12, 13, 14, 15, 16, 17,\n",
       "                                                    18, 19, 20, 21, 22, 23, 24,\n",
       "                                                    25, 26, 27, 28, 29, 30, 31, ...],\n",
       "                                remove_poor_atts=False, split_confidence=1e-12,\n",
       "                                split_criterion='hellinger',\n",
       "                                stop_mem_management=False, tie_threshold=0.05)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pCat_features_idx = list(range(2,43))\n",
    "\n",
    "# Setup the File Stream\n",
    "stream = FileStream(\"../data_output/export_dataframe_0v5.csv\", -1, 1,  pCat_features_idx)\n",
    "#stream = FileStream(\"../data_output/export_dataframe_balanced_v5.csv\", -1, 1,  pCat_features_idx)\n",
    "X, y = stream.next_sample(1000000)\n",
    "print (np.unique(y,return_counts=True) )\n",
    "y_weights = compute_y_weight(y)\n",
    "\n",
    "cfiers = [HoeffdingTreeClassifier(nominal_attributes=pCat_features_idx,split_confidence=1e-12,grace_period=2000,split_criterion='hellinger',binary_split=True), \n",
    "          HoeffdingAdaptiveTreeClassifier(nominal_attributes=pCat_features_idx,split_confidence=1e-12,grace_period=2000,split_criterion='hellinger',binary_split=True)\n",
    "         ]\n",
    "\n",
    "cfiers[0].partial_fit(X,y,[0,1],y_weights )\n",
    "cfiers[1].partial_fit(X,y,[0,1],y_weights )\n",
    "#print(cfiers[0].get_description())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original 0: (array([0, 1]), array([776619, 134890]))\n",
      "prediction 0: (array([0, 1]), array([911435,     74]))\n",
      "precision 0: (array([False,  True]), array([134952, 776557]))\n",
      "tn: 776551, fp: 68, fn: 134884, tp: 6\n",
      "precision 0: 0.08108108108108109\n"
     ]
    }
   ],
   "source": [
    "y_pred_original_0 = cfiers[0].predict(X)\n",
    "print(\"original 0:\", np.unique(y,return_counts=True) )\n",
    "print(\"prediction 0:\", np.unique(y_pred_original_0,return_counts=True) )\n",
    "print (\"precision 0:\",np.unique((y==y_pred_original_0),return_counts=True))\n",
    "\n",
    "#print(cfiers[0].get_model_description())\n",
    "#(confusion_matrix(y, y_pred_original_0, labels=[0,1]))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred_original_0).ravel()\n",
    "print (\"tn: {}, fp: {}, fn: {}, tp: {}\".format(tn, fp, fn, tp))\n",
    "print (\"precision 0:\",tp/(tp+fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original 1: (array([0, 1]), array([776619, 134890]))\n",
      "prediction 1: (array([0, 1]), array([911285,    224]))\n",
      "tn: 776396, fp: 223, fn: 134889, tp: 1\n",
      "precision 1: 0.004464285714285714\n",
      "recall 1: 7.413447994662317e-06\n"
     ]
    }
   ],
   "source": [
    "y_pred_original_1 = cfiers[1].predict(X)\n",
    "print(\"original 1:\", np.unique(y,return_counts=True) )\n",
    "print(\"prediction 1:\", np.unique(y_pred_original_1,return_counts=True) )\n",
    "\n",
    "#print (confusion_matrix(y, y_pred_original_1, labels=[0,1]))\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred_original_1).ravel()\n",
    "print (\"tn: {}, fp: {}, fn: {}, tp: {}\".format(tn, fp, fn, tp))\n",
    "print (\"precision 1:\",tp/(tp+fp))\n",
    "print (\"recall 1:\",tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(cfiers[0].predict(X),return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data_output/export_dataframe_0v5.csv',header=None,names=dataset_feature_names)\n",
    "#df = pd.read_csv('../data_output/export_dataframe_balanced_v5.csv',header=None,names=dataset_feature_names)\n",
    "oracle = cfiers[1]\n",
    "column_label = 'social_class'\n",
    "minority_label = 1\n",
    "nb_samples = 10000\n",
    "max_iterations = 30\n",
    "\n",
    "minority_df = df.loc[df['social_class'] == 1]\n",
    "#print (\"minority\",np.unique(minority_df,return_counts=True))\n",
    "ay=np.asarray(minority_df['social_class'])\n",
    "aX=np.asarray(minority_df.iloc[:,0:-1])\n",
    "\n",
    "\n",
    "sampled_df = generate_syntethic_data (nb_samples,X,y,oracle,column_label,max_iterations)\n",
    "\n",
    "#sampled_df = generate_uniform_adnostic_sample_from_original(X,y,oracle,minority_label,original=True)\n",
    "#np.unique(cfiers[0].predict(X_synthetic),return_counts=True)\n",
    "print (\"Final \",sampled_df.shape)\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pCat_features_idx = list(range(2,43))\n",
    "\n",
    "\n",
    "X_copy=np.asarray(sampled_df.iloc[:,0:-1])\n",
    "y_copy=np.asarray(sampled_df.iloc[:,-1])\n",
    "y_weights_copy = compute_y_weight(y_copy)\n",
    "\n",
    "cfiers_copy = [HoeffdingTreeClassifier(nominal_attributes=pCat_features_idx,split_confidence=1e-10, split_criterion='misclassification'), \n",
    "          HoeffdingAdaptiveTreeClassifier(nominal_attributes=pCat_features_idx,split_confidence=1e-10,split_criterion='misclassification')\n",
    "         ]\n",
    "\n",
    "cfiers_copy[0].partial_fit(X_copy,y_copy,[0,1],y_weights_copy )\n",
    "cfiers_copy[1].partial_fit(X_copy,y_copy,[0,1],y_weights_copy )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfiers_copy[0].get_model_description())\n",
    "print(cfiers_copy[0].measure_byte_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfiers_copy[1].get_model_description())\n",
    "print(cfiers_copy[1].measure_byte_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction 1 copy :\", np.unique(cfiers_copy[0].predict(X),return_counts=True) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction 2 copy :\", np.unique(cfiers_copy[1].predict(X),return_counts=True) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skmultiflow_dev",
   "language": "python",
   "name": "skmultiflow_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
