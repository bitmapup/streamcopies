{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Este notebook funciona con los datos v6 (7 variables) y con el HT de Irene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/irene/Documents/Projects/dual/scripts/methods/\n",
      "/opt/anaconda3/envs/skmultiflow_dev/lib/python36.zip\n",
      "/opt/anaconda3/envs/skmultiflow_dev/lib/python3.6\n",
      "/opt/anaconda3/envs/skmultiflow_dev/lib/python3.6/lib-dynload\n",
      "\n",
      "/Users/MiguelNunez/.local/lib/python3.6/site-packages\n",
      "/opt/anaconda3/envs/skmultiflow_dev/lib/python3.6/site-packages\n",
      "/opt/anaconda3/envs/skmultiflow_dev/lib/python3.6/site-packages/aeosa\n",
      "/opt/anaconda3/envs/skmultiflow_dev/lib/python3.6/site-packages/scikit_multiflow-0.5.dev0-py3.6-macosx-10.9-x86_64.egg\n",
      "/opt/anaconda3/envs/skmultiflow_dev/lib/python3.6/site-packages/IPython/extensions\n",
      "/Users/MiguelNunez/.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "from methods import HoeffdingTree \n",
    "from sklearn.metrics import confusion_matrix\n",
    "print ('\\n'.join(sys.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_uniform_adnostic_sample():\n",
    "    \n",
    "    dict_aux = {}\n",
    "    mu = 0.0 \n",
    "    sigma = 1.0\n",
    "    aux = random.uniform(mu, sigma)\n",
    "    dict_aux['amount_usd']= aux*-1 if (aux<0) else aux\n",
    "    aux = random.uniform(mu, sigma)\n",
    "    dict_aux['client_age']= aux*-1 if (aux<0) else aux\n",
    "    \n",
    "    aux = random.uniform(mu, sigma)\n",
    "    dict_aux['client_gender']=1 if (int(round(aux,0)) == 1 ) else 0\n",
    "    \n",
    "    aux = random.uniform(mu, sigma)\n",
    "    dict_aux['debit_type']=1 if (int(round(aux,0)) == 1 ) else 0\n",
    "        \n",
    "    aux = random.uniform(mu, sigma)\n",
    "    dict_aux['agency_region']= int(round(aux * 12,0))\n",
    "    \n",
    "    aux = random.uniform(mu, sigma)        \n",
    "    dict_aux['merchant_departement']= int(round(aux * 25,0))\n",
    "    \n",
    "    aux = random.uniform(mu, sigma)        \n",
    "    dict_aux['coicop']= int(round(aux * 12,0))\n",
    "        \n",
    "    return dict_aux\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_sample_from_original(pX,py,oracle,minority_label,original=False):\n",
    "    size = len(py)\n",
    "    #size = 2000\n",
    "    print(\"generate_gaussian_original minority nb: \",size )    \n",
    "    i = 0\n",
    "    #mylist = []\n",
    "    X_aux = []\n",
    "    \n",
    "    while (i<size):\n",
    "        \n",
    "        row = pX[i].copy()\n",
    "        #print (type(row))\n",
    "        i+=1\n",
    "        if (not original):\n",
    "            #print (row,i)\n",
    "            if (np.random.randint(2) == 1):\n",
    "                mu = 0.006903\n",
    "                sigma = 0.017028\n",
    "                aux = random.uniform(mu, sigma)\n",
    "                row[0] += aux\n",
    "            elif (np.random.randint(2) == 1):\n",
    "                mu = 0.006903\n",
    "                sigma = 0.017028\n",
    "                aux = random.uniform(mu, sigma)\n",
    "                row[0] += aux\n",
    "                mu = 0.343192\n",
    "                sigma = 0.144008\n",
    "                aux = random.uniform( sigma,mu)\n",
    "                row[1] += aux\n",
    "            else:\n",
    "                mu = 0.343192\n",
    "                sigma = 0.144008\n",
    "                #aux = random.gauss(mu, sigma)\n",
    "                aux = random.uniform( sigma,mu)\n",
    "                row[1] += aux\n",
    "                \n",
    "        \n",
    "        y_iter = oracle.predict( np.asarray( [row] ) )\n",
    "\n",
    "        if (y_iter[0]  == 1):       \n",
    "            #print (\"prediction: \",y_iter,i)\n",
    "            row = np.append(row,[1],axis=0)\n",
    "            X_aux.append (row)\n",
    "    #y_res = oracle.predict( np.asarray(X_aux) )\n",
    "    #print (\"oracle.predict(X) : \", y_res)\n",
    "    #print (np.unique(y_res,return_counts=True) )\n",
    "    return pd.DataFrame(X_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_sample_from_original(pX,py,oracle,minority_label,original=False):\n",
    "    xmin = np.min(pX,axis=0)\n",
    "    xman = np.max(pX,axis=0)\n",
    "    x0_min =  xmin[0]\n",
    "    x1_min =  xmin[1]\n",
    "    x0_max =  xman[0]\n",
    "    x1_max =  xman[1] \n",
    "    print (x0_min,x1_min,x0_max,x1_max)\n",
    "    size = len(py)\n",
    "    #size = 2000\n",
    "    print(\"generate_uniform_from_original minority nb: \",size )    \n",
    "    i = 0\n",
    "    #mylist = []\n",
    "    X_aux = []\n",
    "    \n",
    "    while (i<size):\n",
    "        \n",
    "        row = pX[i].copy()\n",
    "        #print (type(row))\n",
    "        i+=1\n",
    "        if (not original):\n",
    "            #print (row,i)\n",
    "            if (np.random.randint(2) == 1):\n",
    "                aux = random.uniform(x0_min, x0_max)\n",
    "                row[0] += aux\n",
    "            elif (np.random.randint(2) == 1):\n",
    "                aux = random.uniform(x0_min, x0_max)\n",
    "                row[0] += aux\n",
    "                aux = random.uniform( x1_min,x1_max)\n",
    "                row[1] += aux\n",
    "            else:\n",
    "                aux = random.uniform( x1_min,x1_max)\n",
    "                row[1] += aux\n",
    "                \n",
    "        \n",
    "        y_iter = oracle.predict( np.asarray( [row] ) )\n",
    "        \n",
    "        if (y_iter[0]  == 1):       \n",
    "            #print (\"prediction: \",y_iter,i)\n",
    "            X_aux.append (row)\n",
    "    #y_res = oracle.predict( np.asarray(X_aux) )\n",
    "    #print (\"oracle.predict(X) : \", y_res)\n",
    "    #print (np.unique(y_res,return_counts=True) )\n",
    "    return pd.DataFrame(X_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_syntethic_data (nb_samples,pX,py,oracle,minority_label,pmax_iterations=10):\n",
    "    \n",
    "    newdf = pd.DataFrame()\n",
    "    \n",
    "    #Minority class\n",
    "    print (\"generating....\")\n",
    "    sampled_df = generate_gaussian_sample_from_original(pX,py,oracle,pmax_iterations)\n",
    "    nb_sampled = sampled_df.shape[0]\n",
    "    print(\"nb_sampled: \",sampled_df.shape)\n",
    "    \n",
    "    ii = 0\n",
    "    print (\"while....\")\n",
    "    if (sampled_df.shape[0]<=(nb_samples/2)):\n",
    "        #print (sampled_df.shape[0],\" < \",(nb_samples/2))\n",
    "        #print (\"max_iterations: \",pmax_iterations)\n",
    "        while   (ii < pmax_iterations):    \n",
    "            #print ('i : ',ii)\n",
    "            sampled_df_aux = generate_gaussian_sample_from_original(pX,py,oracle,pmax_iterations)\n",
    "            sampled_df = sampled_df.append(sampled_df_aux, ignore_index = True) \n",
    "            #print (sampled_df_aux.shape)\n",
    "            #print (sampled_df.shape)\n",
    "            ii+=1\n",
    "        if (sampled_df.shape[0]<=(nb_samples/2)):\n",
    "            ii = 0\n",
    "            print (\"second sampling\")\n",
    "            while   (ii < pmax_iterations  ):\n",
    "                sampled_df_aux = generate_gaussian_sample_from_original(pX,py,oracle,pmax_iterations,original=True)\n",
    "                sampled_df = sampled_df.append(sampled_df_aux, ignore_index = False) \n",
    "                print (sampled_df.shape)\n",
    "                ii+=1\n",
    "                if (sampled_df.shape[0]>=(nb_samples/2)):\n",
    "                    print (\"Break : \", sampled_df.shape)\n",
    "                    break\n",
    "    if (sampled_df.shape[0] >= int(nb_samples/2) ):\n",
    "        newdf = sampled_df.iloc[0:int(nb_samples/2)]#, ignore_index = True)\n",
    "    else :\n",
    "        newdf = sampled_df\n",
    "    column_names = ['amount_usd',\n",
    "                    'client_age',\n",
    "                    'client_gender',     \n",
    "                    'debit_type',        \n",
    "                    'agency_region',     \n",
    "                    'merchant_departement',\n",
    "                   'coicop',\n",
    "                   'social_class']\n",
    "    newdf.columns = column_names\n",
    "    print (\"newdf shape\", newdf.shape)\n",
    "    print (\"newdf cols\", newdf.columns)\n",
    "    \n",
    "    #All classes\n",
    "    \n",
    "    i = 0\n",
    "    column_names = ['amount_usd',\n",
    "                    'client_age',\n",
    "                    'client_gender',     \n",
    "                    'debit_type',        \n",
    "                    'agency_region',     \n",
    "                    'merchant_departement',\n",
    "                   'coicop']\n",
    "    df = pd.DataFrame(columns = column_names)\n",
    "    while (i < (nb_samples/2)):\n",
    "        row ={}\n",
    "        row = generate_uniform_adnostic_sample()\n",
    "        df = df.append(row  , ignore_index=True)\n",
    "        i+=1\n",
    "  \n",
    "    aX=np.asarray(df)\n",
    "    aY=oracle.predict(aX)\n",
    "    df['social_class']=aY\n",
    "    \n",
    "    print (\"df shape\", df.shape)\n",
    "    print (\"df cols\", df.columns)\n",
    "    \n",
    "    newdf = newdf.append(df)\n",
    "    print (\"newdf shape\", newdf.columns)\n",
    "    print (\"newdf cols\", newdf.columns)\n",
    "    newdf = newdf.sample(frac = 1)\n",
    "    \n",
    "    return newdf #sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv('../data_output/export_dataframe_0v6.csv',header=None,names=dataset_feature_names)\n",
    "oracle = cfiers[1]\n",
    "column_label = 'social_class'\n",
    "minority_label = 1\n",
    "nb_samples = 10000\n",
    "max_iterations = 30\n",
    "\n",
    "minority_df = df.loc[df['social_class'] == 1]\n",
    "#print (\"minority\",np.unique(minority_df,return_counts=True))\n",
    "ay=np.asarray(minority_df['social_class'])\n",
    "aX=np.asarray(minority_df.iloc[:,0:-1])\n",
    "\n",
    "\n",
    "sampled_df = generate_syntethic_data (nb_samples,X,y,oracle,column_label,max_iterations)\n",
    "\n",
    "#sampled_df = generate_uniform_adnostic_sample_from_original(X,y,oracle,minority_label,original=True)\n",
    "#np.unique(cfiers[0].predict(X_synthetic),return_counts=True)\n",
    "print (\"Final \",sampled_df.shape)\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkRate(y):\n",
    "    rate = -1\n",
    "    count = np.unique(y,return_counts=True)\n",
    "    #print(\"y\",y)\n",
    "    #print(\"count\",count)\n",
    "    #print(\"count len\",len(count[0]) )\n",
    "    if (len(count) >= 1):\n",
    "        if (len(count[0]) == 1):\n",
    "            classall   = count[0]\n",
    "            classA     = classall[0]\n",
    "            classB     = 0\n",
    "            classCount = count[1]\n",
    "            classCountA= classCount[0]\n",
    "            classCountB= 0\n",
    "        else:\n",
    "            classall   = count[0]\n",
    "            classA     = classall[0]\n",
    "            classB     = classall[1]\n",
    "            classCount = count[1]\n",
    "            classCountA= classCount[0]\n",
    "            classCountB= classCount[1]\n",
    "            \n",
    "        if (classCountA<classCountB): # classCountA Minoritary class\n",
    "            rate = float(classCountA/(classCountA+classCountB))\n",
    "        else: # classCountB Minoritary class\n",
    "            rate = float(classCountB/(classCountA+classCountB))\n",
    "    \n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMetrics (confusionMatrix):\n",
    "    tn, fp, fn, tp = confusionMatrix.ravel()\n",
    "    precision = tp/(tp+fp)\n",
    "    recall    = tp/(tp+fn)\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows processed: 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2a618026627a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mcfiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mno_first_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chanchan/streamcopies/3_Copies/methods.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \"\"\"\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHoeffdingTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chanchan/streamcopies/3_Copies/base_HoeffdingTree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_partial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chanchan/streamcopies/3_Copies/base_HoeffdingTree.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m#self.tree_.extend(node._split(self.split_function_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_best_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;31m#idx, thr = fast.best_split(node.X.astype('float'), node.y.astype('float'), self.n_classes_, self.n_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chanchan/streamcopies/3_Copies/base_HoeffdingTree.py\u001b[0m in \u001b[0;36m_best_split\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;31m#gini_left = 1.0 - sum((num_left[x] / i) ** 2 for x in range(self.n_classes_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;31m#gini_right = 1.0 - sum((num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0mmisclassification_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_left\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0mmisclassification_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_right\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = []\n",
    "no_first_model = False\n",
    "numrows = 0\n",
    "f= open(\"metrics_hti.csv\",\"w+\")\n",
    "minorityClassRate = 0.12\n",
    "min_windows_size = 10000\n",
    "X_ = []\n",
    "y_ = []\n",
    "last_X = []\n",
    "last_y = []\n",
    "i = 0\n",
    "cfiers = HoeffdingTree(split_criterion=\"mean\")\n",
    "\n",
    "f.write(\"size_win,precision,recall,count,tn, fp, fn, tp\\n\")\n",
    "with open(\"../data_output/export_dataframe_0v6.csv\") as infile:\n",
    "    for line in infile:\n",
    "        numrows += 1\n",
    "        aux_list= (line.split(\",\"))\n",
    "        #print (\"aux_list\",aux_list)\n",
    "        aux_y = int(aux_list.pop())\n",
    "        aux_x = np.array(aux_list.copy(),dtype=np.float32) \n",
    "        y_.append(aux_y)        \n",
    "        X_.append(aux_x)\n",
    "        i += 1\n",
    "        #print (\"y_\",y_)\n",
    "        #print (\"checkRate(y_) {} >= {} minorityClassRate: {}\".format( checkRate(y_),  minorityClassRate, (checkRate(y_) >= minorityClassRate)))\n",
    "        if ( (checkRate(y_) >= minorityClassRate) and len(y_)>=min_windows_size ):\n",
    "            print (\"Rows processed: {}\".format(numrows))\n",
    "            \n",
    "            X = np.asarray(X_).copy()\n",
    "            y = np.asarray(y_).copy()\n",
    "            \n",
    "            # Fit HT models\n",
    "            if (no_first_model):\n",
    "                # Semi copy\n",
    "                minority_label = 1\n",
    "                nb_samples = 10000\n",
    "                max_iterations = 30\n",
    "                oracle = models[-1]\n",
    "                column_label = 'social_class'\n",
    "                sampled_df = generate_syntethic_data (nb_samples,last_X,last_y,oracle,column_label,max_iterations)\n",
    "                sampled_y=np.asarray(sampled_df['social_class'])\n",
    "                sampled_X=np.asarray(sampled_df.iloc[:,0:-1])\n",
    "                y = np.concatenate((sampled_y, y), axis=0)\n",
    "                X = np.concatenate((sampled_X, X), axis=0)\n",
    "                # Learn\n",
    "                cfiers.fit(X,y)\n",
    "                models.append(cfiers)\n",
    "                \n",
    "                if (len(models)>=3):\n",
    "                    cfiers = models[-2]\n",
    "                    yPred0 = cfiers.predict(X)\n",
    "                    cm0 = confusion_matrix(y,yPred0)\n",
    "                    p0,r0 = computeMetrics(cm0)\n",
    "                    tn, fp, fn, tp = cm0.ravel()\n",
    "                    f.write(\"{},{},{},{},{},{},{},{}\\n\".format(i,p0,r0,str(np.unique(yPred0,return_counts=True)),tn, fp, fn, tp ))\n",
    "                    f.flush()\n",
    "                i = 0\n",
    "                last_X = X\n",
    "                last_y = y\n",
    "                X = None\n",
    "                y = None\n",
    "                X_ = []\n",
    "                y_ = []\n",
    "            else:\n",
    "                cfiers.fit(X,y)\n",
    "                models.append(cfiers)\n",
    "                no_first_model = True\n",
    "                i = 0\n",
    "                last_X = X\n",
    "                last_y = y\n",
    "                X = None\n",
    "                y = None\n",
    "                X_ = []\n",
    "                y_ = []\n",
    "            \n",
    "    X = np.asarray(X_).copy()\n",
    "    y = np.asarray(y_).copy()\n",
    "    \n",
    "    if (len(models)>=3):\n",
    "                    cfiers = models[-2]\n",
    "                    yPred0 = cfiers.predict(X)\n",
    "                    cm0 = confusion_matrix(y,yPred0)\n",
    "                    p0,r0 = computeMetrics(cm0)\n",
    "                    tn, fp, fn, tp = cm0.ravel()\n",
    "                    f.write(\"{},{},{},{},{},{},{},{}\\n\".format(i,p0,r0,str(np.unique(yPred0,return_counts=True)),tn, fp, fn, tp ))\n",
    "                    f.flush()\n",
    "    i = 0\n",
    "    X = None\n",
    "    y = None\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    f.write(\"Total rows processed: {}\".format(numrows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skmultiflow_dev",
   "language": "python",
   "name": "skmultiflow_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
