{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import sys\n",
    "import pprint\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#sys.path.append(\"../Mfig/\")\n",
    "#import mplp\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import date, timedelta\n",
    "from datetime import datetime\n",
    "from scipy.stats import entropy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conectionDB():\n",
    "    conn_string = \"dbname='stic' port='5432' user='stic' password='stic2019' host='gpmaster.as-dell.copernic.local'\"\n",
    "    #print (\"Connecting to database ->{}\".format(conn_string))\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dpto = dict()\n",
    "dict_province = dict()\n",
    "dict_district = dict()\n",
    "with open('../data/ubigeo_inei.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        #print (row)\n",
    "        if (str(row[0])!=''):\n",
    "            dict_dpto[str(row[3]).lower()]=(str(row[0]))\n",
    "        if (str(row[1])!=''):\n",
    "            dict_province[(row[4]).lower()]=(row[0]+''+row[1])\n",
    "        if (str(row[2])!=''):\n",
    "            dict_district[(row[5]).lower()]=(row[0]+''+row[1]+''+row[2])\n",
    "        \n",
    "dict_gender = {'M':1,'F':0,'':-1}\n",
    "dict_card = {'TD':1,'TC':0,'':-1}\n",
    "dict_region = {'NORTE':0,'ORIENTE':1,'SUR':2,'CENTRO':3,'SIERRA CENTRAL':4,\n",
    "               'LIMA CENTRO':5,'LIMA ESTE':6,'LIMA SUR':7,'LIMA NORTE':8,\n",
    "               'LIMA MODERNA':9,'LIMA PROVINCIA':10,'CALLAO':11,'':12}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames=[ \n",
    "        'social_class' ,     #-- 0\n",
    "        'country_code',      #-- 1\n",
    "        'amount_usd',        #-- 2 *\n",
    "        'client_age',        #-- 3 *\n",
    "        'client_gender',     #-- 4\n",
    "        'debit_type',        #-- 5 Credit or debit card\n",
    "        'agency_region',     #-- 9\n",
    "        'merchant_departement', #-- 10\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all (nb_records = 100, class_label=0):\n",
    "    PLIMIT = nb_records\n",
    "    \n",
    "    sqlStr = \"\"\n",
    "    if (class_label == 1):\n",
    "        sqlStr = \" AND social_class <= 3 \"\n",
    "    else:\n",
    "        sqlStr = \" AND social_class > 3 \"\n",
    "    \n",
    "    print (\"Processing class {} number of records: {} ...\".format(class_label,PLIMIT))\n",
    "    query =\"\"\"\n",
    "        SELECT \n",
    "            social_class,      -- 0\n",
    "            amount_usd,        -- 1 *\n",
    "            client_age,        -- 2 *\n",
    "            client_gender,     -- 3\n",
    "            debit_type,        -- 4 Credit or debit card\n",
    "            agency_region,     -- 5\n",
    "            merchant_departement -- 10\n",
    "        FROM \n",
    "            public.bbva \n",
    "        INNER JOIN \n",
    "            client\n",
    "        ON \n",
    "            client.client_id = bbva.client_id\n",
    "        WHERE\n",
    "            bbva.merchant_id != '00000000' {}\n",
    "        ORDER BY \n",
    "            bbva.date ASC\n",
    "        LIMIT {}\n",
    "    ;\n",
    "    \"\"\".format(sqlStr,PLIMIT)\n",
    "    conn = conectionDB()\n",
    "    cursor = conn.cursor();\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.fillna(-1, inplace=True)\n",
    "\n",
    "    df['small_social_class'] = df.apply(lambda row: 1 if (row.social_class<=3) else 0 , axis=1)\n",
    "    del df['social_class']\n",
    "\n",
    "    df=df.replace({ \"merchant_departement\":dict_dpto,\n",
    "                    \"agency_region\": dict_region\n",
    "                     })\n",
    "    df.fillna(-1, inplace=True)\n",
    "    \n",
    "    ###############################        \n",
    "    #  Normalize numeric columns  #\n",
    "    ###############################\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    # Create x, where x the 'scores' column's values as floats\n",
    "    x_amount_usd = df[['amount_usd']].values.astype(float)\n",
    "    x_client_age = df[['client_age']].values.astype(float)\n",
    "\n",
    "    # Create an object to transform the data to fit minmax processor\n",
    "    x_scaled_amount_usd = min_max_scaler.fit_transform(x_amount_usd)\n",
    "    x_scaled_client_age = min_max_scaler.fit_transform(x_client_age)\n",
    "\n",
    "    # Run the normalizer on the dataframe\n",
    "    df_normalized_amount_usd = pd.DataFrame(x_scaled_amount_usd)\n",
    "    df_normalized_client_age = pd.DataFrame(x_scaled_client_age)\n",
    "\n",
    "    #######################################        \n",
    "    #  One hot enconding numeric columns  #\n",
    "    #######################################\n",
    "\n",
    "    df_client_gender_henc = pd.get_dummies(df['client_gender'], prefix = 'client_gender_')\n",
    "    df_debit_type_henc = pd.get_dummies(df['debit_type'], prefix = 'debit_type_')\n",
    "    df_agency_region_henc = pd.get_dummies(df['agency_region'], prefix = 'agency_region_')\n",
    "    df_merchant_departement_henc = pd.get_dummies(df['merchant_departement'], prefix = 'merchant_departement_')\n",
    "\n",
    "    newdf = pd.concat([df_normalized_amount_usd, \n",
    "               df_normalized_client_age,\n",
    "               df_client_gender_henc,\n",
    "               df_debit_type_henc,\n",
    "               df_agency_region_henc,\n",
    "               df_merchant_departement_henc\n",
    "              ], axis=1)\n",
    "    newdf['social_class'] = df['small_social_class']\n",
    "    \n",
    "\n",
    "    newdf.to_csv (r'../data_output/export_dataframe_v4_oneclass_{}.csv'.format(class_label), index = None, header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class 1 number of records: 500000 ...\n"
     ]
    }
   ],
   "source": [
    "#extract_all(500000,0)\n",
    "extract_all(500000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlineHT",
   "language": "python",
   "name": "onlineht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
